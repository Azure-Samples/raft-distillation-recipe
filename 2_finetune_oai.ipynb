{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RAFT GPT-4o-mini fine tuning\n",
    "\n",
    "Now that we've created labelled training data, we can fine tune our model using the Supervised Fine Tuning technique. Azure OpenAI uses LoRA to fine tune models efficiently. **LoRA (Low-Rank Adaptation)** finetuning of a Large Language Model is a technique used to adapt pre-trained language models to specific tasks efficiently and with fewer computational resources.  \n",
    "\n",
    " Instead of adjusting all the model parameters, LoRA introduces a small number of additional parameters (low-rank matrices) that modify the model's behavior. These new parameters are trained while keeping the original model's parameters mostly unchanged. This way, the model can learn the new task without the need for extensive computational resources or time.\n",
    "\n",
    " Azure OpenAI lets developers customize OpenAI models with their own data and easily deploy their custom model using an easy to use and affordable managed service.\n",
    "\n",
    " While Fine Tuning can be a complex process, Azure OpenAI abstracts away a lot of the complexity to make fine tuning accessible to any developer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. How much will this cost?\n",
    "\n",
    "Fine tuning pricing on Azure OpenAI makes fine tuning experiment cost very predictable. Training pricing is based on the number of tokens you're training your model on. Therefore, it is very easy to predict and manage the cost of your finetuning experiments.\n",
    "\n",
    "For GPT-4o mini, training price is $0.003300 per 1K tokens.\n",
    "\n",
    "So to estimate the cost of our fine tuning job we can use the following formula\n",
    "\n",
    "`(Training cost per 1K input tokens / 1K) * number of tokens in input file * number of epochs trained`\n",
    "\n",
    "**epoch:** a complete iteration through a dataset during the training process of a process, \n",
    "\n",
    "1. If the number of epochs is too low: Your model might be underfitted, which means it could perform poorly because it hasn't learned enough from the training data. In essence, it may not have had enough iterations to effectively learn and adjust its parameters (e.g., weights and biases).\n",
    "\n",
    "2. If the number of epochs is too high: There's a risk of overfitting, where the model becomes too specialized in the training data and performs poorly on unseen data (examples that werenâ€™t in your training dataset).\n",
    "\n",
    "the number of epochs is a parameter of the fine tuning job, usually 3 epochs is a reasonable number\n",
    "\n",
    "**Let's explore our dataset and estimate our fine tuning costs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Variables passed by previous notebooks\n",
    "load_dotenv(\".env.state\")\n",
    "\n",
    "ds_name = os.getenv(\"DATASET_NAME\")\n",
    "ds_path = f\"dataset/{ds_name}\"\n",
    "dataset_path_ft_train = f\"{ds_path}-files/{ds_name}-ft.train.jsonl\"\n",
    "dataset_path_ft_valid = f\"{ds_path}-files/{ds_name}-ft.valid.jsonl\"\n",
    "\n",
    "print(f\"Using dataset {ds_name} for fine tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import json\n",
    "\n",
    "# loading environment variables\n",
    "load_dotenv(\".env.state\")\n",
    "\n",
    "training_file_path = dataset_path_ft_train\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"role\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "with open(training_file_path, 'r', encoding='utf-8') as f:\n",
    "    num_tokens=0\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "    messages = [ d.get('messages') for d in dataset]\n",
    "    for message in messages:\n",
    "        \n",
    "        num_tokens += num_tokens_from_messages(message)\n",
    "    \n",
    "print(f\"Number of tokens in training data: {num_tokens}\")\n",
    "\n",
    "training_cost_per_token = 0.003300 / 1000\n",
    "num_epochs = 3\n",
    "total_cost = num_tokens * training_cost_per_token * num_epochs\n",
    "\n",
    "print(f\"Total estimated cost for training: {total_cost:.2f} USD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Uploading the training and validation data to Azure OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.identity import get_bearer_token_provider\n",
    "\n",
    "aoai_endpoint = os.getenv(\"FINETUNED_AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "# Authenticate using the default Azure credential chain\n",
    "azure_credential = DefaultAzureCredential()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = aoai_endpoint,\n",
    "  api_version = \"2024-05-01-preview\",  # This API version or later is required to access seed/events/checkpoint features\n",
    "  azure_ad_token_provider = get_bearer_token_provider(\n",
    "    azure_credential, \"https://cognitiveservices.azure.com/.default\"\n",
    "  )\n",
    ")\n",
    "\n",
    "validation_file_path = dataset_path_ft_valid\n",
    "\n",
    "# Upload the training and validation dataset files to Azure OpenAI with the SDK.\n",
    "training_response = client.files.create(\n",
    "    file = open(training_file_path, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = training_response.id\n",
    "\n",
    "validation_response = client.files.create(\n",
    "    file = open(validation_file_path, \"rb\"), purpose=\"fine-tune\"\n",
    ")\n",
    "validation_file_id = validation_response.id\n",
    "\n",
    "print(\"Training file ID:\", training_file_id)\n",
    "print(\"Validation file ID:\", validation_file_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Creating the fine tuning job\n",
    "\n",
    "For each fine tuning job, you can specify the following hyperparameters. \n",
    "\n",
    "- epochs: An \"epoch\" is a term used to describe one complete pass through the entire training dataset\n",
    "- learning rate multiplier: this will be used as the learning rate for the fine tuning job, as a multiple of the model's original learning rate. We recommend experimenting with values in the range 0.02 to 0.2 to see what produces the best results\n",
    "- batch size:  how many training examples you use at one time during training, common choices are (32, 64, 128, 256). This value is to be tuned based on the size of your data and available compute.\n",
    "\n",
    "The general recommendation is to initially train without specifying any of these, Azure OpenAI will pick a default for you based on dataset size, then adjusting based on results to find the ideal combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit fine-tuning training job\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "    training_file = training_file_id,\n",
    "    validation_file = validation_file_id,\n",
    "    model = \"gpt-4o-mini\", # Enter base model name. Note that in Azure OpenAI the model name contains dashes and cannot contain dot/period characters.\n",
    "    seed = 105 # seed parameter controls reproducibility of the fine-tuning job. If no seed is specified one will be generated automatically.\n",
    ")\n",
    "\n",
    "job_id = response.id\n",
    "\n",
    "# You can use the job ID to monitor the status of the fine-tuning job.\n",
    "# The fine-tuning job will take some time to start and complete.\n",
    "\n",
    "print(\"Job ID:\", response.id)\n",
    "print(\"Status:\", response.status)\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import update_state\n",
    "\n",
    "update_state(\"FINETUNED_OPENAI_JOB_ID\", response.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
